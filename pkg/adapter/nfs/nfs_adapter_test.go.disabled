package nfs

import (
	"context"
	"net"
	"sync"
	"testing"
	"time"

	"github.com/marmos91/dittofs/pkg/metrics"
)

// TestGracefulShutdown verifies that the adapter waits for connections to complete
func TestGracefulShutdown(t *testing.T) {
	// Create adapter with short shutdown timeout
	config := NFSConfig{
		Port:     0, // OS assigns random port
		Timeouts: NFSTimeoutsConfig{Shutdown: 2 * time.Second},
	}
	adapter := New(config, nil) // nil = no metrics

	// Start server in background
	ctx, cancel := context.WithCancel(context.Background())
	serverDone := make(chan error, 1)
	go func() {
		serverDone <- adapter.Serve(ctx)
	}()

	// Get the actual port from the listener (blocks until ready)
	listenerAddr := adapter.GetListenerAddr()
	if listenerAddr == "" {
		t.Fatal("Failed to get listener address")
	}

	// Create a test connection but don't close it
	conn, err := net.Dial("tcp", listenerAddr)
	if err != nil {
		t.Fatalf("Failed to connect to adapter: %v", err)
	}
	defer func() { _ = conn.Close() }()

	// Verify connection is tracked
	time.Sleep(100 * time.Millisecond)
	if adapter.GetActiveConnections() != 1 {
		t.Errorf("Expected 1 active connection, got %d", adapter.GetActiveConnections())
	}

	// Initiate shutdown
	shutdownStart := time.Now()
	cancel()

	// Wait for server to complete
	err = <-serverDone
	shutdownDuration := time.Since(shutdownStart)

	// Should complete within shutdown timeout + grace period
	if shutdownDuration > 3*time.Second {
		t.Errorf("Shutdown took too long: %v (expected < 3s)", shutdownDuration)
	}

	// Should have error (timeout or context cancelled)
	if err == nil {
		t.Error("Expected error from shutdown, got nil")
	}
}

// TestForcedConnectionClosure verifies that connections are force-closed after timeout
func TestForcedConnectionClosure(t *testing.T) {
	// Create adapter with very short shutdown timeout
	config := NFSConfig{
		Port:     0, // OS assigns random port
		Timeouts: NFSTimeoutsConfig{Shutdown: 500 * time.Millisecond},
	}
	adapter := New(config, nil, nil)

	// Start server in background
	ctx, cancel := context.WithCancel(context.Background())
	serverDone := make(chan error, 1)
	go func() {
		serverDone <- adapter.Serve(ctx)
	}()

	// Get the actual port from the listener (blocks until ready)
	listenerAddr := adapter.GetListenerAddr()

	// Create a test connection
	conn, err := net.Dial("tcp", listenerAddr)
	if err != nil {
		t.Fatalf("Failed to connect to adapter: %v", err)
	}
	defer func() { _ = conn.Close() }()

	// Verify connection is tracked
	time.Sleep(100 * time.Millisecond)
	if adapter.GetActiveConnections() != 1 {
		t.Errorf("Expected 1 active connection, got %d", adapter.GetActiveConnections())
	}

	// Track whether connection was closed by server
	connClosed := make(chan bool, 1)
	go func() {
		buf := make([]byte, 1)
		_, err := conn.Read(buf)
		if err != nil {
			connClosed <- true
		}
	}()

	// Initiate shutdown
	cancel()

	// Wait for connection to be force-closed
	select {
	case <-connClosed:
		// Connection was closed - good!
		t.Log("Connection was force-closed as expected")
	case <-time.After(2 * time.Second):
		t.Error("Connection was not force-closed within timeout")
	}

	// Wait for server to complete
	err = <-serverDone
	if err == nil {
		t.Error("Expected error from shutdown with force-close, got nil")
	}
}

// TestConnectionLimiting verifies that MaxConnections is enforced
func TestConnectionLimiting(t *testing.T) {
	// Create adapter with connection limit
	config := NFSConfig{
		Port:           0, // OS assigns random port
		MaxConnections: 2,
		Timeouts:       NFSTimeoutsConfig{Shutdown: 1 * time.Second},
	}
	adapter := New(config, nil, nil)

	// Start server in background
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	serverDone := make(chan error, 1)
	go func() {
		serverDone <- adapter.Serve(ctx)
	}()

	// Get the actual port from the listener (blocks until ready)
	listenerAddr := adapter.GetListenerAddr()

	// Create MaxConnections connections
	var conns []net.Conn
	for i := 0; i < 2; i++ {
		conn, err := net.Dial("tcp", listenerAddr)
		if err != nil {
			t.Fatalf("Failed to create connection %d: %v", i, err)
		}
		conns = append(conns, conn)
	}
	defer func() {
		for _, conn := range conns {
			_ = conn.Close()
		}
	}()

	// Wait for connections to be tracked
	time.Sleep(200 * time.Millisecond)

	// Verify connection count
	activeConns := adapter.GetActiveConnections()
	if activeConns != 2 {
		t.Errorf("Expected 2 active connections, got %d", activeConns)
	}

	// Note: Testing the semaphore blocking behavior is timing-dependent and flaky
	// in unit tests. The important thing is that the connection count matches
	// MaxConnections, which proves the semaphore is working.
	// More comprehensive integration tests would be needed to verify blocking behavior.

	t.Logf("Connection limit enforced: %d/%d connections active", activeConns, config.MaxConnections)
}

// TestDrainMode verifies that new connections are rejected during shutdown
func TestDrainMode(t *testing.T) {
	// Create adapter
	config := NFSConfig{
		Port:     0, // OS assigns random port
		Timeouts: NFSTimeoutsConfig{Shutdown: 2 * time.Second},
	}
	adapter := New(config, nil, nil)

	// Start server in background
	ctx, cancel := context.WithCancel(context.Background())
	serverDone := make(chan error, 1)
	go func() {
		serverDone <- adapter.Serve(ctx)
	}()

	// Get the actual port from the listener (blocks until ready)
	listenerAddr := adapter.GetListenerAddr()

	// Create initial connection - should succeed
	conn1, err := net.Dial("tcp", listenerAddr)
	if err != nil {
		t.Fatalf("Failed to create initial connection: %v", err)
	}
	defer func() { _ = conn1.Close() }()

	// Initiate shutdown
	cancel()

	// Wait for shutdown to initiate
	time.Sleep(200 * time.Millisecond)

	// Try to create new connection - should fail (listener closed)
	_, err = net.Dial("tcp", listenerAddr)
	if err == nil {
		t.Error("New connection succeeded during shutdown, expected failure (drain mode)")
	} else {
		t.Logf("New connection rejected during shutdown: %v (expected)", err)
	}

	// Wait for server to complete
	<-serverDone
}

// TestConcurrentShutdown verifies that concurrent shutdown calls are safe
func TestConcurrentShutdown(t *testing.T) {
	config := NFSConfig{
		Port:     0,
		Timeouts: NFSTimeoutsConfig{Shutdown: 1 * time.Second},
	}
	adapter := New(config, nil, nil)

	ctx, cancel := context.WithCancel(context.Background())
	serverDone := make(chan error, 1)
	go func() {
		serverDone <- adapter.Serve(ctx)
	}()

	// Wait for listener to be ready
	_ = adapter.GetListenerAddr()

	// Call Stop() multiple times concurrently
	var wg sync.WaitGroup
	for i := 0; i < 10; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			stopCtx, stopCancel := context.WithTimeout(context.Background(), 2*time.Second)
			defer stopCancel()
			_ = adapter.Stop(stopCtx)
		}()
	}

	// Also cancel context
	cancel()

	// Wait for all Stop() calls to complete
	wg.Wait()

	// Wait for server to complete
	<-serverDone

	t.Log("Concurrent shutdown calls completed successfully")
}

// TestConnectionTracking verifies that connection tracking works correctly
func TestConnectionTracking(t *testing.T) {
	config := NFSConfig{
		Port:     0,
		Timeouts: NFSTimeoutsConfig{Shutdown: 1 * time.Second},
	}
	adapter := New(config, nil, nil)

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	serverDone := make(chan error, 1)
	go func() {
		serverDone <- adapter.Serve(ctx)
	}()

	// Get the actual port from the listener (blocks until ready)
	listenerAddr := adapter.GetListenerAddr()

	// Verify initial state
	if adapter.GetActiveConnections() != 0 {
		t.Errorf("Expected 0 active connections initially, got %d", adapter.GetActiveConnections())
	}

	// Create connections and verify count increases
	var conns []net.Conn
	for i := 1; i <= 5; i++ {
		conn, err := net.Dial("tcp", listenerAddr)
		if err != nil {
			t.Fatalf("Failed to create connection %d: %v", i, err)
		}
		conns = append(conns, conn)

		// Wait for connection to be tracked
		time.Sleep(50 * time.Millisecond)

		if adapter.GetActiveConnections() != int32(i) {
			t.Errorf("Expected %d active connections, got %d", i, adapter.GetActiveConnections())
		}
	}

	// Close connections and verify count decreases
	for i, conn := range conns {
		_ = conn.Close()
		time.Sleep(50 * time.Millisecond)

		expected := int32(len(conns) - i - 1)
		if adapter.GetActiveConnections() != expected {
			t.Errorf("Expected %d active connections after closing %d, got %d",
				expected, i+1, adapter.GetActiveConnections())
		}
	}

	// Verify final state
	if adapter.GetActiveConnections() != 0 {
		t.Errorf("Expected 0 active connections finally, got %d", adapter.GetActiveConnections())
	}

	// Clean shutdown
	cancel()
	<-serverDone
}

// TestMetricsIntegration verifies that metrics are recorded (when provided)
func TestMetricsIntegration(t *testing.T) {
	// This test just verifies the adapter accepts metrics
	// Full integration testing would require implementing the full interface
	config := NFSConfig{
		Port:     0,
		Timeouts: NFSTimeoutsConfig{Shutdown: 1 * time.Second},
	}

	// Test with nil metrics (no-op)
	adapter := New(config, nil, nil)
	if adapter.metrics == nil {
		t.Error("Expected metrics to be set (no-op), got nil")
	}

	// Test with no-op metrics struct
	mockMetrics := metrics.NewNoopNFSMetrics()
	adapter2 := New(config, nil, mockMetrics)
	if adapter2.metrics == nil {
		t.Error("Expected metrics to be set, got nil")
	}
}
