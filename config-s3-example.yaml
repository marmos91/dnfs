# DittoFS Configuration with S3 Content Store
#
# This example shows how to configure DittoFS with Amazon S3 or S3-compatible
# storage (MinIO, Wasabi, Backblaze B2, etc.) for content storage.
#
# Usage:
#   dittofs --config config-s3-example.yaml

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  level: INFO          # Log level: DEBUG, INFO, WARN, ERROR
  format: text         # Log format: text or json
  output: stdout       # Output: stdout, stderr, or file path

# =============================================================================
# Server Configuration
# =============================================================================
server:
  shutdown_timeout: 30s  # Graceful shutdown timeout

# =============================================================================
# Content Store Configuration - S3
# =============================================================================
#
# S3 Path-Based Design:
#   The S3 content store uses relative file paths as object keys, mirroring the
#   filesystem structure in the S3 bucket. This enables:
#     - Easy inspection of S3 contents (human-readable keys)
#     - Metadata reconstruction from S3 (disaster recovery)
#     - Simple migration and backup strategies
#
#   ContentID Format: "shareName/path/to/file"
#     - Leading "/" removed from share name and path
#     - No ":content" suffix
#     - Share name included as root prefix
#
#   Example:
#     Share Name: /export
#     NFS Path: /export/documents/report.pdf
#     ContentID: export/documents/report.pdf
#     S3 Key: export/documents/report.pdf
#     (or with prefix: dittofs/export/documents/report.pdf)
#
content:
  type: s3  # Use S3 content store

  s3:
    # -------------------------------------------------------------------------
    # Required Settings
    # -------------------------------------------------------------------------

    # AWS Region (required)
    region: us-east-1

    # S3 Bucket name (required, must exist)
    bucket: dittofs-content

    # -------------------------------------------------------------------------
    # Optional Settings
    # -------------------------------------------------------------------------

    # Key prefix for all objects (optional)
    # Useful for sharing bucket with other applications or namespacing
    # Example: "dittofs/" results in keys like "dittofs/export/documents/report.pdf"
    key_prefix: ""

    # Custom endpoint for S3-compatible storage (optional)
    # Leave empty to use AWS S3
    # Example: Cubbit DS3: "https://s3.cubbit.eu"
    endpoint: ""

    # AWS credentials (optional)
    # If empty, uses AWS credential chain:
    #   1. Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
    #   2. Shared credentials file (~/.aws/credentials)
    #   3. IAM role (for EC2 instances)
    access_key_id: ""
    secret_access_key: ""

    # Multipart upload part size in bytes (optional)
    # Default: 10485760 (10MB)
    # Range: 5242880 (5MB) to 5368709120 (5GB)
    # Larger parts = faster uploads but more memory usage
    part_size: 10485760

# =============================================================================
# Metadata Store Configuration
# =============================================================================
metadata:
  type: badger  # Use BadgerDB for persistent metadata

  badger:
    db_path: /tmp/dittofs-metadata  # Path to BadgerDB database

  capabilities:
    max_filename_length: 255
    max_path_length: 4096
    max_file_size: 1099511627776  # 1TB
    max_link_count: 65535
    max_symlink_depth: 40
    supports_sparse_files: true
    supports_hard_links: false
    supports_symlinks: false
    supports_extended_attributes: false
    supports_file_locking: false
    case_sensitive: true
    case_preserving: true

# =============================================================================
# Shares Configuration
# =============================================================================
shares:
  - name: /export
    read_only: false
    async: false

    identity_mapping:
      map_all_to_anonymous: false
      map_privileged_to_anonymous: false
      anonymous_uid: 65534
      anonymous_gid: 65534

    root_attr:
      mode: 0755
      uid: 0
      gid: 0

# =============================================================================
# Protocol Adapters
# =============================================================================
adapters:
  nfs:
    enabled: true
    port: 2049

    # Mount protocol settings
    mount:
      enabled: true

    # NFSv3 settings
    v3:
      enabled: true
      max_read_size: 1048576     # 1MB
      max_write_size: 1048576    # 1MB
      preferred_read_size: 131072   # 128KB
      preferred_write_size: 131072  # 128KB
      preferred_readdir_size: 32768 # 32KB

# =============================================================================
# S3 Configuration Examples for Different Providers
# =============================================================================

# --- Amazon S3 ---
# content:
#   type: s3
#   s3:
#     region: us-east-1
#     bucket: my-dittofs-bucket
#     # Uses AWS credential chain (environment, ~/.aws/credentials, IAM role)

# --- Cubbit DS3 (S3-compatible distributed storage) ---
# content:
#   type: s3
#   s3:
#     region: us-east-1
#     bucket: my-dittofs-bucket
#     endpoint: https://s3.cubbit.eu
#     access_key_id: YOUR_CUBBIT_ACCESS_KEY
#     secret_access_key: YOUR_CUBBIT_SECRET_KEY
